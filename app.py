#!/usr/python3
import streamlit as st
import chromadb 
from pypdf import PdfReader
from openai import OpenAI
import os

# ------------é¡µé¢é…ç½®----------

st.set_page_config(page_title="pdfæ™ºèƒ½é—®ç­”åŠ©æ‰‹",layout="wide")
st.title("ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹")

api_keys = st.text_input("è¯·è¾“å…¥api_key",type="password")

file_paths = st.file_uploader("æ–‡ä»¶ä¸Šä¼ æŒ‰é’®",type=["pdf"])

process_btn = st.button("å¼€å§‹å¤„ç†å…¥åº“")

st.divider()


#API_KEY = os.getenv("ALIYUN_API_KEY") 


# =================  æ ¸å¿ƒå‡½æ•°åŒº (é€»è¾‘å±‚) =================
# [Cç¨‹åºå‘˜å¿…è¯»] @st.cache_resource
# è¿™æ˜¯ä¸€ä¸ª"è£…é¥°å™¨"ã€‚å®ƒçš„ä½œç”¨ç±»ä¼¼äº C è¯­è¨€é‡Œçš„ "static" å˜é‡æˆ–å•ä¾‹æ¨¡å¼ã€‚
# å®ƒå‘Šè¯‰ Streamlitï¼š"è¿™ä¸ªæ•°æ®åº“è¿æ¥åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œä¸è¦æ¯æ¬¡ç½‘é¡µåˆ·æ–°éƒ½é‡æ–°è¿ä¸€éã€‚"
@st.cache_resource
def init_db():
    #åˆå§‹åŒ–æ•°æ®åº“
    # PersistentClient ä¼šè‡ªåŠ¨åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ 'my_db' æ¥å­˜æ•°æ®
    print("æ­£åœ¨è¿æ¥æœ¬åœ°çŸ¥è¯†åº“ (./my_db)...")
    db_client = chromadb.PersistentClient(path="./my_db")

    # åˆ›å»ºä¸€ä¸ª"é›†åˆ" (Collection)ï¼Œç±»ä¼¼äº SQL é‡Œçš„"è¡¨"
    # get_or_create è¡¨ç¤ºï¼šå¦‚æœè¡¨å­˜åœ¨å°±è¯»å–ï¼Œä¸å­˜åœ¨å°±æ–°å»º
    collection = db_client.get_or_create_collection(name = "manual_docs")
    return collection

'''ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹'''
def extract_text_from_pdf(file_path):
    '''ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹'''
      
    try:
        reader = PdfReader(file_path)
        print(f"æ­£åœ¨è¯»å–PDF")
        print(f"PDFçš„é¡µæ•°ä¸ºï¼š{len(reader.pages)}")

        full_text = ""

    #å¾ªç¯éå†
    # enumerate å¸®æˆ‘ä»¬åŒæ—¶è·å–é¡µç (i)å’Œé¡µé¢å¯¹è±¡(page)
        for i,page in enumerate(reader.pages):
        #æå–æ–‡å­—
            text = page.extract_text()

            if  text:
                full_text += text + "\n" 
                print(f" -ç¬¬{i+1}é¡µæå–äº†{len(text)}ä¸ªå­—ç¬¦")
            else:
                print(f" - ç¬¬ {i+1} é¡µä¼¼ä¹æ˜¯çº¯å›¾ç‰‡ï¼Œæ— æ³•æå–æ–‡å­—")

        return full_text
    except Exception as e:
            print(f"è§£æå¤±è´¥ï¼š{e}")
            return None

'''å°†æ€»æ–‡å­—åˆ‡ç‰‡'''
def split_text_into_chunks(text,chunk_size = 500,overlap = 50):

    """
    æŠŠé•¿æ–‡æœ¬åˆ‡æˆå°å—
    :param text: å®Œæ•´çš„é•¿æ–‡æœ¬
    :param chunk_size: æ¯å—å¤§çº¦å¤šå°‘å­—
    :param overlap: é‡å éƒ¨åˆ† (é˜²æ­¢ä¸€å¥è¯æ­£å¥½è¢«åˆ‡ä¸¤åŠ)
    :return: åˆ‡å¥½çš„æ–‡æœ¬åˆ—è¡¨ list[str]
    """   
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)

        start += (chunk_size - overlap)
    
    return chunks

'''1.é…ç½®é˜¿é‡Œäº‘,è¾“å…¥api_keyæ‰åˆå§‹åŒ–'''
client_ai = None
if api_keys:
    client_ai = OpenAI(
        api_key=api_keys, 
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
'''2.è°ƒç”¨é˜¿é‡Œäº‘ç”Ÿæˆå‘é‡'''
def get_embedding(text):
    response = client_ai.embeddings.create(
        input= text,
        model="text-embedding-v3"
    )
    return response.data[0].embedding

#--------------------ä¸šåŠ¡é€»è¾‘åŒº (æ§åˆ¶å±‚) =================
#åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
collection = init_db()

if process_btn and file_paths and api_keys:
    with st.spinner("æ­£åœ¨æ¸…æ´—æ•°æ®å…¥åº“ã€‚ã€‚ã€‚"):
        #1.è¯»æ–‡å­—
        raw_text = extract_text_from_pdf(file_paths)

        #2.åˆ‡ç‰‡
        chunks = split_text_into_chunks(raw_text)

        #3.å‘é‡åŒ–å…¥åº“
        process_bar = st.progress(0)

        ids = []
        embeddings = []

        for i,chunk in enumerate(chunks):
            vec = get_embedding(chunk)
            if vec :
                ids.append(f"id{i}")#ç´¢å¼•åšid
                embeddings.append(vec)
            #æ›´æ–°è¿›åº¦æ¡
            process_bar.progress((i+1)/len(chunks))
        
        if embeddings:
            collection.add(documents=chunks,embeddings=embeddings,ids=ids)
            st.success(f"æˆåŠŸå…¥åº“ï¼Œå…±ä¼ å…¥{len(chunks)}ä¸ªç‰‡æ®µ")
        else:
            st.error(f"æ•°æ®å¤„ç†å¤±è´¥ï¼Œæœªèƒ½ç”Ÿæˆå‘é‡ï¼")
        
# [Cç¨‹åºå‘˜å¿…è¯»] st.session_state
# Streamlit æ¯æ¬¡äº¤äº’ï¼ˆæ¯”å¦‚ç‚¹æŒ‰é’®ï¼‰éƒ½ä¼šä»å¤´è¿è¡Œæ•´ä¸ªè„šæœ¬ã€‚
# å±€éƒ¨å˜é‡ä¼šé‡ç½®ã€‚å¦‚æœä½ æƒ³"è®°ä½"ä¹‹å‰çš„èŠå¤©è®°å½•ï¼Œå¿…é¡»å­˜åœ¨ session_state é‡Œã€‚
# è¿™ç±»ä¼¼äº C è¯­è¨€é‡Œçš„ "å…¨å±€å˜é‡" æˆ– "å †å†…å­˜"ã€‚

if "messages" not in st.session_state:
    st.session_state.messages = []

#1.æŠŠå†å²è®°å½•ç”»å‡ºæ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 2. ç­‰å¾…ç”¨æˆ·è¾“å…¥
# := æµ·è±¡è¿ç®—ç¬¦ï¼ŒCè¯­è¨€é‡Œæ²¡æœ‰ã€‚æ„æ€æ˜¯ï¼šèµ‹å€¼å¹¶åˆ¤æ–­æ˜¯å¦éç©º
if prompt :=st.chat_input("è¯·æ ¹æ®æ‰‹å†Œæé—®ã€‚ã€‚"):
    if not api_keys:
        st.warning("è¯·å…ˆè¾“å…¥api key")
        st.stop()

    with st.chat_message("user"):
        st.markdown(prompt)
    #è®°åœ¨æœ¬å­ä¸Š
    st.session_state.messages.append({"role":"user","content":prompt})

    # --- RAG æ ¸å¿ƒæ£€ç´¢é€»è¾‘ ---
    q_vec = get_embedding(prompt)
    if q_vec:
        results = collection.query(query_embeddings=[q_vec],n_results=5)

        if results['documents'] and results['documents'][0]:
            doc_list = results['documents'][0]
            best_context = "\n\n=======\n\n".join(doc_list)

            # åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºä¸ªå°æŠ˜å æ¡†ï¼Œå‘Šè¯‰ç”¨æˆ·å‚è€ƒäº†å“ªæ®µåŸæ–‡ (Debugç”¨)
            with st.expander("ğŸ” AI å‚è€ƒäº†ä»¥ä¸‹ 5 ä¸ªåŸæ–‡ç‰‡æ®µ (Debug)"):
                st.info(best_context)
            
            messages_history = []
            for msg in st.session_state.messages:
                messages_history.append({"role":msg["role"],"content":msg["content"]})

            #è°ƒç”¨å¤§æ¨¡å‹
            full_prompt = f"åŸºäºæ­¤çŸ¥è¯†ï¼š\n{best_context}\n\nå›ç­”ç”¨æˆ·çš„é—®é¢˜:{prompt}"
            messages_history.append({"role":"user","content":full_prompt})
            with st.chat_message("assistant"):
                stream = client_ai.chat.completions.create(
                    model="qwen3-max",
                    messages=messages_history,
                    stream=True # å¼€å¯æ‰“å­—æœºæµå¼æ•ˆæœ
                                                           
                )
                response = st.write_stream(stream)
            # è®°ä¸‹ AI çš„å›å¤
            st.session_state.messages.append({"role": "assistant", "content": response})    
        else:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")










